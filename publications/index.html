<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Renato  Panda | publications</title>
<meta name="description" content="A simple webpage describing my work and life.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->




    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Renato   Panda</span>
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/honors/">
                honors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/press/">
                press
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications by categories in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TAFFC</abbr>
    
  
  </div>

  <div id="Panda2020" class="col-sm-8">
    
      <div class="title">Audio Features for Music Emotion Recognition: a Survey</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo Manuel</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Affective Computing</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/9229494/" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Panda, Malheiro, Paiva - 2020 - Audio Features for Music Emotion Recognition a Survey.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The design of meaningful audio features is a key need to advance the state-of-the-art in Music Emotion Recognition (MER). This work presents a survey on the existing emotionally-relevant computational audio features, supported by the music psychology literature on the relations between eight musical dimensions (melody, harmony, rhythm, dynamics, tone color, expressivity, texture and form) and specific emotions. Based on this review, current gaps and needs are identified and strategies for future research on feature engineering for MER are proposed, namely ideas for computational audio features that capture elements of musical form, texture and expressivity that should be further researched. Finally, although the focus of this article is on classical feature engineering methodologies (based on handcrafted features), perspectives on deep learning-based approaches are discussed.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TAFFC</abbr>
    
  
  </div>

  <div id="Panda2018a" class="col-sm-8">
    
      <div class="title">Novel Audio Features for Music Emotion Recognition</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Affective Computing</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/8327886/" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Panda, Malheiro, Paiva - 2020 - Novel Audio Features for Music Emotion Recognition.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell’s emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4% (by 9%), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD</abbr>
    
  
  </div>

  <div id="Panda2019" class="col-sm-8">
    
      <div class="title">Emotion-based Analysis and Classification of Audio Music</div>
      <div class="author">
        
          
            
              <em>Panda, Renato</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://hdl.handle.net/10316/87618" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Panda - 2019 - Emotion-based Analysis and Classification of Audio Music.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This research work addresses the problem of music emotion recognition using audio signals. Music emotion recognition research has been gaining ground over the last two decades. In it, the typical approach starts with a dataset, composed of music files and associated emotion ratings given by listeners. This data, typically audio signals, is first processed by computational algorithms in order to extract and summarize their characteristics, known as features (e.g., beats per minute, spectral metrics). Next, the feature set is fed to machine learning algorithms looking for patterns that connect them to the given emotional annotations. As a result, a computational model is created, which is able to infer the emotion of a new and unlabelled music file based on the previously found patterns. Although several studies have been published, two main issues remain open and are the current barrier to progress in field. First, a high-quality public and sizeable audio dataset is needed, which can be widely adopted as a standard and used by different works. Currently, the public available ones suffer from known issues such as low quality annotations or limited size. Also, we believe novel emotionally-relevant audio features are needed to overcome the plateau of the last years. Supporting this idea is the fact that the vast majority of previous works were focused on the computational classification component, typically using a similar set of audio features originally proposed to tackle other audio analysis problems (e.g., speech recognition). Our work focuses on these two problems. Proposing novel emotionally-relevant audio features requires knowledge from several fields. Thus, our work started with a review of music and emotion literature to understand how emotions can be described and classified, how music and music dimensions work and, as a final point, to merge both fields by reviewing the identified relations between musical dimensions and emotional responses. Next, we reviewed the existent audio features, relating them with one of the eight musical dimensions: melody, harmony, rhythm, dynamics, tone color, expressive techniques, musical texture and musical form. As a result, we observed that audio features are unbalanced across musical dimensions, with expressive techniques, musical texture and form said to be emotionally-relevant but lacking audio extractors. To address the abovementioned issues, we propose several audio features. These were built on previous work to estimate the main melody notes from the low-level audio signals. Next, various musically-related metrics were extracted, e.g., glissando presence, articulation information, changes in dynamics and others. To assess their relevance to emotion recognition, a dataset containing 900 audio clips, annotated in four classes (Russell’s quadrants) was built. Our experimental results show that the proposed features are emotionally-relevant and their inclusion in emotion recognition models leads to better results. Moreover, we also measured the influence of both existing and novel features, leading to a better understanding of how different musical dimensions influence specific emotion quad-rants. Such results give us insights about the open issues and help us define possible research paths to the near future.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WF-IoT</abbr>
    
  
  </div>

  <div id="Mendes2019" class="col-sm-8">
    
      <div class="title">VITASENIOR-MT: A distributed and scalable cloud-based telehealth solution</div>
      <div class="author">
        
          
            
              
                
                  Mendes, Diogo,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Dias, Pedro,
                
              
            
          
        
          
            
              
                
                  Jorge, Dário,
                
              
            
          
        
          
            
              
                
                  António, Ricardo,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Luis_Oliveira10" target="_blank">Oliveira, Luis</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://home.isr.uc.pt/~gpires/" target="_blank">Pires, Gabriel</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE 5th World Forum on Internet of Things</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Mendes et al. - 2019 - VITASENIOR-MT A distributed and scalable cloud-based telehealth solution.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>VITASENIOR-MT is a telehealth platform that allows to remotely monitor biometric and environmental data in a domestic environment, designed specifically to the elderly population. This paper proposes a highly scalable and efficient architecture to transport, process, store and visualize the data collected by devices of an Internet of Things (IoT) scenario. The cloud infrastructure follows a microservices architecture to provide computational scalability, better fault isolation, easy integration and automatic deployment. This solution is complemented with a pre-processing and validation of the collected data at the edge of the Internet by using the Fog Computing concept, allowing a better computing distribution. The presented approach provides personal data security and a simplified way to collect and present the data to the different actors, allowing a dynamic and intuitive management of patients and equipment to caregivers. The presented load tests proved that this solution is more efficient than a monolithic approach, promoting better access and control in the data flowing from heterogeneous equipment.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HealthCom</abbr>
    
  
  </div>

  <div id="Pires2018" class="col-sm-8">
    
      <div class="title">VITASENIOR-MT: a telehealth solution for the elderly focused on the interaction with TV</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://home.isr.uc.pt/~gpires/" target="_blank">Pires, Gabriel</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Pedro_Correia9" target="_blank">Correia, Pedro</a>,
                
              
            
          
        
          
            
              
                
                  Jorge, Dário,
                
              
            
          
        
          
            
              
                
                  Mendes, Diogo,
                
              
            
          
        
          
            
              
                
                  Gomes, Nelson,
                
              
            
          
        
          
            
              
                
                  Dias, Pedro,
                
              
            
          
        
          
            
              
                
                  Ferreira, Pedro,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Ana-Lopes-2" target="_blank">Lopes, Ana</a>,
                
              
            
          
        
          
            
              
                
                  Manso, António,
                
              
            
          
        
          
            
              
                
                  <a href="https://home.isr.uc.pt/~laa/" target="_blank">Almeida, Luís</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Luis_Oliveira10" target="_blank">Oliveira, Luís</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Monteiro, Paulo,
                
              
            
          
        
          
            
              
                
                  Grácio, Carla,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.researchgate.net/profile/Telmo_Pereira2" target="_blank">Pereira, Telmo</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 20th IEEE International Conference on e-Health Networking, Application &amp; Services - Healthcom2018</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Pires et al. - 2018 - VITASENIOR-MT a telehealth solution for the elderly focused on the interaction with TV.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Remote monitoring of health parameters is a promising approach to improve the health condition and quality of life of particular groups of the population, which can also alleviate the current expenditure and demands of healthcare systems. The elderly, usually affected by chronic comorbidities, are a specific group of the population that can strongly benefit from telehealth technologies, allowing them to reach a more independent life, by living longer in their own homes. Usability of telehealth technologies and their acceptance by end-users are essential requirements for the success of telehealth implementation. Older people are resistant to new technologies or have difficulty in using them due to vision, hearing, sensory and cognition impairments. In this paper, we describe the implementation of an IoT-based telehealth solution designed specifically to address the elderly needs. The end-user interacts with a TV-set to record biometric parameters, and to receive warning and recommendations related to health and environmental sensor recordings. The familiarization of older people with the TV is expected to provide a more user-friendly interaction ensuring the effectiveness integration of the end-user in the overall telehealth solution.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TAFFC</abbr>
    
  
  </div>

  <div id="Malheiro2018" class="col-sm-8">
    
      <div class="title">Emotionally-Relevant Features for Classification and Regression of Music Lyrics</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Gomes, Paulo,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Affective Computing – TAFFC</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://ieeexplore.ieee.org/document/7536113/" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Malheiro et al. - 2018 - Emotionally-Relevant Features for Classification and Regression of Music Lyrics.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This research addresses the role of lyrics in the music emotion recognition process. Our approach is based on several state of the art features complemented by novel stylistic, structural and semantic features. To evaluate our approach, we created a ground truth dataset containing 180 song lyrics, according to Russell’s emotion model. We conduct four types of experiments: regression and classification by quadrant, arousal and valence categories. Comparing to the state of the art features (ngrams - baseline), adding other features, including novel features, improved the F-measure from 69.9%, 82.7% and 85.6% to 80.1%, 88.3% and 90%, respectively for the three classification experiments. To study the relation between features and emotions (quadrants) we performed experiments to identify the best features that allow to describe and discriminate each quadrant. To further validate these experiments, we built a validation set comprising 771 lyrics extracted from the AllMusic platform, having achieved 73.6% F-measure in the classification by quadrants. We also conducted experiments to identify interpretable rules that show the relation between features and emotions and the relation among features. Regarding regression, results show that, comparing to similar studies for audio, we achieve a similar performance for arousal and a much better performance for valence.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">GESTEC</abbr>
    
  
  </div>

  <div id="Pires2018a" class="col-sm-8">
    
      <div class="title">VITASENIOR–MT: Architecture of a Telehealth Solution</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://home.isr.uc.pt/~gpires/" target="_blank">Pires, Gabriel</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Ana-Lopes-2" target="_blank">Lopes, Ana</a>,
                
              
            
          
        
          
            
              
                
                  Manso, António,
                
              
            
          
        
          
            
              
                
                  Jorge, Dário,
                
              
            
          
        
          
            
              
                
                  Mendes, Diogo,
                
              
            
          
        
          
            
              
                
                  <a href="https://home.isr.uc.pt/~laa/" target="_blank">Almeida, Luís</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Luis_Oliveira10" target="_blank">Oliveira, Luís</a>,
                
              
            
          
        
          
            
              
                
                  Gomes, Nelson,
                
              
            
          
        
          
            
              
                
                  Dias, Pedro,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Telmo_Pereira2" target="_blank">Pereira, Telmo</a>,
                
              
            
          
        
          
            
              
                
                  Monteiro, Paulo,
                
              
            
          
        
          
            
              
                
                  and Grácio, Carla
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Gestão &amp; Tecnologi@ - Criação de Valor em Saúde (GESTEC)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://apoioinformatico4.wixsite.com/conferencia-2018/programacao" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Pires et al. - 2018 - VITASENIOR–MT Architecture of a Telehealth Solution.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>VITASENIOR-MT is a telehealth solution under development that aims to monitor and improve the healthcare of elderly people living in the region of Médio Tejo. This solution performs both remote and local monitoring of biometric parameters of the elderly, and also of environmental parameters of their homes. The biometric variables include heart rate and temperature measurements collected automatically, by means of a bracelet, throughout the day. Blood pressure, body weight, and other biometric parameters are measured on a daily basis by the senior’s own initiative, and automatically recorded. The environmental parameters include temperature, carbon monoxide and carbon dioxide measurements. A TV set is used as a mean of interaction between the user and the medical devices. The TV set is also used to receive medical warnings and recommendations according to clinical profiles, and to receive environmental alerts. All data and alerts can be accessible to senior’s family and healthcare providers. In alarm situations, an automatic operational procedure will be triggered establishing communication to predefined entities.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISMIR</abbr>
    
  
  </div>

  <div id="Panda2018" class="col-sm-8">
    
      <div class="title">Musical Texture and Expressivity Features for Music Emotion Recognition</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 19th International Society for Music Information Retrieval Conference – ISMIR 2018</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Malheiro, Paiva - 2018 - Musical Texture and Expressivity Features for Music Emotion Recognition.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a set of novel emotionally-relevant audio fea- tures to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regard- ing emotion and music was conducted, to understand how the various music concepts may influence human emo- tions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied mu- sical concepts. The intersection of this data showed an un- balanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and ex- pressive techniques are lacking. Based on this, we devel- oped a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public da- taset containing 900 30-second clips, annotated in terms of Russell’s emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector ma- chines and 20 repetitions of 10-fold cross-validation.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">KDIR</abbr>
    
  
  </div>

  <div id="Malheiro2016a" class="col-sm-8">
    
      <div class="title">Classification and Regression of Music Lyrics: Emotionally-Significant Features</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Gomes, Paulo,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 8th International Conference on Knowledge Discovery and Information Retrieval – KDIR 2016</em>
      
      
        2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Malheiro et al. - 2016 - Classification and Regression of Music Lyrics Emotionally-Significant Features.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This research addresses the role of lyrics in the music emotion recognition process. Our approach is based on several state of the art features complemented by novel stylistic, structural and semantic features. To evaluate our approach, we created a ground truth dataset containing 180 song lyrics, according to Russell’s emotion model. We conduct four types of experiments: regression and classification by quadrant, arousal and valence categories. Comparing to the state of the art features (ngrams-baseline), adding other features, including novel features, improved the F-measure from 68.2%, 79.6% and 84.2% to 77.1%, 86.3% and 89.2%, respectively for the three classification experiments. To study the relation between features and emotions (quadrants) we performed experiments to identify the best features that allow to describe and discriminate between arousal hemispheres and valence meridians. To further validate these experiments, we built a validation set comprising 771 lyrics extracted from the AllMusic platform, having achieved 73.6% Fmeasure in the classification by quadrants. Regarding regression, results show that, comparing to similar studies for audio, we achieve a similar performance for arousal and a much better performance for valence.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECML/PKDD</abbr>
    
  
  </div>

  <div id="Malheiro2016" class="col-sm-8">
    
      <div class="title">Bi-modal music emotion recognition: Novel lyrical features and dataset</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Gomes, Paulo,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 9th International Workshop on Music and Machine Learning – MML 2016 – in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases – ECML/PKDD 2016</em>
      
      
        2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Malheiro et al. - 2016 - Bi-modal music emotion recognition Novel lyrical features and dataset.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This research addresses the role of audio and lyrics in the music emo- tion recognition. Each dimension (e.g., audio) was separately studied, as well as in a context of bimodal analysis. We perform classification by quadrant catego- ries (4 classes). Our approach is based on several audio and lyrics state-of-the-art features, as well as novel lyric features. To evaluate our approach we create a ground-truth dataset. The main conclusions show that unlike most of the similar works, lyrics performed better than audio. This suggests the importance of the new proposed lyric features and that bimodal analysis is always better than each dimension.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAI</abbr>
    
  
  </div>

  <div id="Panda2015" class="col-sm-8">
    
      <div class="title">Music Emotion Recognition with Standard and Melodic Audio Features</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Bruno_Rocha14" target="_blank">Rocha, Bruno</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Applied Artificial Intelligence – AAI</em>
      
      
        2015
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Rocha, Paiva - 2015 - Music Emotion Recognition with Standard and Melodic Audio Features.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a novel approach to music emotion recognition by combining standard and melodic features extracted directly from audio. To this end, a new audio dataset organized similarly to the one used in MIREX mood task comparison was created. From the data, 253 standard and 98 melodic features are extracted and used with several supervised learning techniques. Results show that, generally, melodic features perform better than standard audio. The best result, 64% f-measure, with only 11 features (9 melodic and 2 standard), was obtained with ReliefF feature selection and Support Vector Machines.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CMMR</abbr>
    
  
  </div>

  <div id="Panda2013" class="col-sm-8">
    
      <div class="title">Dimensional music emotion recognition: Combining standard and melodic audio features</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Bruno_Rocha14" target="_blank">Rocha, Bruno</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 10th International Symposium on Computer Music Multidisciplinary Research – CMMR 2013</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Rocha, Paiva - 2013 - Dimensional music emotion recognition Combining standard and melodic audio features.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose an approach to the dimensional music emotion recognition (MER) problem, combining both standard and melodic audio features. The dataset proposed by Yang is used, which consists of 189 audio clips. From the audio data, 458 standard features and 98 melodic features were extracted. We experimented with several supervised learning and feature selection strategies to evaluate the proposed approach. Employing only standard audio features, the best attained performance was 63.2% and 35.2% for arousal and valence prediction, respectively (R2 statistics). Combining standard audio with melodic features, results improved to 67.4 and 40.6%, for arousal and valence, respectively. To the best of our knowledge, these are the best results attained so far with this dataset.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECML/PKDD</abbr>
    
  
  </div>

  <div id="Rocha2013" class="col-sm-8">
    
      <div class="title">Music Emotion Recognition: The Importance of Melodic Features</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Bruno_Rocha14" target="_blank">Rocha, Bruno</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 6th International Workshop on Music and Machine Learning – MML 2013 – in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases – ECML/PKDD 2013</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Rocha, Panda, Paiva - 2013 - Music Emotion Recognition The Importance of Melodic Features.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the importance of a melodic audio (MA) feature set in music emotion recognition (MER) and compare its performance to an approach using only standard audio (SA) features. We also analyse the fusion of both types of features. Employing only SA features, the best attained performance was 46.3%, while using only MA features the best outcome was 59.1% (F- measure). A combination of SA and MA features improved results to 64%. These results might have an important impact to help break the so-called glass ceiling in MER, as most current approaches are based on SA features.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECML/PKDD</abbr>
    
  
  </div>

  <div id="Malheiro2013" class="col-sm-8">
    
      <div class="title">Music Emotion Recognition from Lyrics: A Comparative Study</div>
      <div class="author">
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  Gomes, Paulo,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 6th International Workshop on Music and Machine Learning – MML 2013 – in conjunction with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases – ECML/PKDD 2013</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Malheiro et al. - 2013 - Music Emotion Recognition from Lyrics A Comparative Study.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a study on music emotion recognition from lyrics. We start from a dataset of 764 samples (audio+lyrics) and perform feature extraction using several natural language processing techniques. Our goal is to build classifiers for the different datasets, comparing different algorithms and using feature selection. The best results (44.2% F-measure) were attained with SVMs. We also perform a bi-modal analysis that combines the best feature sets of audio and lyrics.The combination of the best audio and lyrics features achieved better results than the best feature set from audio only (63.9% F- Measure against 62.4% F-Measure).</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CMMR</abbr>
    
  
  </div>

  <div id="Panda2013a" class="col-sm-8">
    
      <div class="title">Multi-Modal Music Emotion Recognition: A New Dataset, Methodology and Comparative Analysis</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  <a href="https://www.cisuc.uc.pt/en/people/ricardo-malheiro" target="_blank">Malheiro, Ricardo</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Bruno_Rocha14" target="_blank">Rocha, Bruno</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.researchgate.net/profile/Luis_Oliveira10" target="_blank">Oliveira, António Pedro</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 10th International Symposium on Computer Music Multidisciplinary Research – CMMR 2013</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda et al. - 2013 - Multi-Modal Music Emotion Recognition A New Dataset, Methodology and Comparative Analysis.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a multi-modal approach to the music emotion recognition (MER) problem, combining information from distinct sources, namely audio, MIDI and lyrics. We introduce a methodology for the automatic creation of a multi-modal music emotion dataset resorting to the AllMusic database, based on the emotion tags used in the MIREX Mood Classification Task. Then, MIDI files and lyrics corresponding to a sub-set of the obtained audio samples were gathered. The dataset was organized into the same 5 emotion clusters defined in MIREX. From the audio data, 177 standard features and 98 melodic features were extracted. As for MIDI, 320 features were collected. Finally, 26 lyrical features were extracted. We experimented with several supervised learning and feature selection strategies to evaluate the proposed multi-modal approach. Employing only standard audio features, the best attained performance was 44.3% (F-measure). With the multi-modal approach, results improved to 61.1%, using only 19 multi-modal features. Melodic audio features were particularly important to this improvement.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MIREX</abbr>
    
  
  </div>

  <div id="Panda2012a" class="col-sm-8">
    
      <div class="title">MIREX 2012: Mood Classification Tasks Submission</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 8th Music Information Retrieval Exchange – MIREX 2012, as part of the 13th International Society for Music Information Retrieval Conference – ISMIR 2012</em>
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Paiva - 2012 - MIREX 2012 Mood Classification Tasks Submission.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this work, three audio frameworks – Marsyas, MIR Toolbox and PsySound3, were used to extract audio fea-tures from the audio samples. These features are then used to train several classification models, resulting in the different versions submitted to MIREX 2012 mood classi-fication task.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">DAFx</abbr>
    
  
  </div>

  <div id="Panda2012b" class="col-sm-8">
    
      <div class="title">Music Emotion Classification: Dataset Acquisition and Comparative Analysis</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 15th International Conference on Digital Audio Effects – DAFx 2012</em>
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Paiva - 2012 - Music Emotion Classification Dataset Acquisition and Comparative Analysis.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper we present an approach to emotion classification in audio music. The process is conducted with a dataset of 903 clips and mood labels, collected from Allmusic database, organized in five clusters similar to the dataset used in the MIREX Mood Classification Task. Three different audio frameworks - Marsyas, MIR Toolbox and Psysound, were used to extract several features. These audio features and annotations are used with supervised learning techniques to train and test various classifiers based on support vector machines. To access the importance of each feature several different combinations of features, obtained with feature selection algorithms or manually selected were tested. The performance of the solution was measured with 20 repetitions of 10-fold cross validation, achieving a F-measure of 47.2% with precision of 46.8% and recall of 47.6%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MML/ICML</abbr>
    
  
  </div>

  <div id="Panda2012" class="col-sm-8">
    
      <div class="title">Music Emotion Classification: Analysis of a Classifier Ensemble Approach</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 5th International Workshop on Music and Machine Learning – MML 2012 – in conjunction with the 19th International Conference on Machine Learning – ICML 2012</em>
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Paiva - 2012 - Music Emotion Classification Analysis of a Classifier Ensemble Approach.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a five regression models’ system to classify music emotion. To this end, a dataset similar to MIREX contest dataset was used. Songs from each cluster are separated in five sets and labeled as 1. A similar number of songs from other clusters are then added to each set and labeled 0, training regression models to output a value representing how much a song is related to the specific cluster. The five outputs are combined and the highest score used as classification. An F-measure of 68.9% was obtained. Results were validated with 10-fold cross-validation and feature selection was tested.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AES</abbr>
    
  
  </div>

  <div id="Panda2011" class="col-sm-8">
    
      <div class="title">Using Support Vector Machines for Automatic Mood Tracking in Audio Music</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 130th Audio Engineering Society Convention – AES 130</em>
      
      
        2011
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Paiva - 2011 - Using Support Vector Machines for Automatic Mood Tracking in Audio Music.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper we propose a solution for automatic mood tracking in audio music, based on supervised learning and classification. To this end, various music clips with a duration of 25 seconds, previously annotated with arousal and valence (AV) values, were used to train several models. These models were used to predict quadrants of the Thayer’s taxonomy and AV values, of small segments from full songs, revealing the mood changes over time. The system accuracy was measured by calculating the matching ratio between predicted results and full song annotations performed by volunteers. Different combinations of audio features, frameworks and other parameters were tested, resulting in an accuracy of 56.3% and showing there is still much room for improvement.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SMC</abbr>
    
  
  </div>

  <div id="Panda2011a" class="col-sm-8">
    
      <div class="title">Automatic creation of mood playlists in the thayer plane: A methodology and a comparative study</div>
      <div class="author">
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 8th Sound and Music Computing Conference, SMC 2011</em>
      
      
        2011
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Panda, Paiva - 2011 - Automatic creation of mood playlists in the thayer plane A methodology and a comparative study.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose an approach for the automatic creation of mood playlists in the Thayer plane (TP). Music emotion recognition is tackled as a regression and classification problem, aiming to predict the arousal and valence (AV) values of each song in the TP, based on Yang’s dataset. To this end, a high number of audio features are extracted using three frameworks: PsySound, MIR Toolbox and Marsyas. The extracted features and Yang’s annotated AV values are used to train several Support Vector Regressors, each employing different feature sets. The best performance, in terms of R2statistics, was attained after feature selection, reaching 63% for arousal and 35.6% for valence. Based on the predicted location of each song in the TP, mood playlists can be created by specifying a point in the plane, from which the closest songs are retrieved. Using one seed song, the accuracy of the created playlists was 62.3% for 20-song playlists, 24.8% for 5-song playlists and 6.2% for the top song. \textcopyright 2011 Panda and Paiva.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">INForum</abbr>
    
  
  </div>

  <div id="Cardoso2011" class="col-sm-8">
    
      <div class="title">MOODetector: A Prototype Software Tool for Mood-based Playlist Generation</div>
      <div class="author">
        
          
            
              
                
                  Cardoso, Luís,
                
              
            
          
        
          
            
              
                <em>Panda, Renato</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://eden.dei.uc.pt/~ruipedro/" target="_blank">Paiva, Rui Pedro</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 3\textordmasculine Simpósio de Informática – INForum 2011</em>
      
      
        2011
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
      <a href="/assets/pdf/papers/Cardoso, Panda, Paiva - 2011 - MOODetector A Prototype Software Tool for Mood-based Playlist Generation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a prototype software tool for the automatic generation of mood-based playlists. The tool works as typical music player, extended with mechanisms for automatic estimation of arousal and valence values in the Thayer plane (TP). Playlists are generated based on one seed song or a desired mood trajectory path drawn by the user, according to the distance to the seed(s) in the TP. Besides playlist generation, a mood tracking visualization tool is also implemented, where individual songs are segmented and classified according to the quadrants in the TP. Additionally, the methodology for music emotion recognition, tackled in this paper as a regression and classification problem, is described, along with the process for feature extraction and selection. Experimental results for mood regression are slightly higher than the state of the art, indicating the viability of the followed strategy (in terms of R2 statistics, arousal and valence estimation accuracy reached 63% and 35.6%, respectively).</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MSc</abbr>
    
  
  </div>

  <div id="Panda2010" class="col-sm-8">
    
      <div class="title">Automatic Mood Tracking in Audio Music</div>
      <div class="author">
        
          
            
              <em>Panda, Renato</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2010
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://mir.dei.uc.pt/pdf/Theses/MOODetector/Panda MSc Thesis 2010.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    
      <a href="/assets/pdf/papers/Panda - 2010 - Automatic Mood Tracking in Audio Music.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this thesis, music emotion recognition is approached, having detection of mood changes over a song as the main focus. The first semester served entirely to gain knowledge on the area and a to produce an initial design. This strong theoretical knowledge was further developed during the second semester, serving as a starting point to build a music analysis Qt application, based on the Thayer’s model of mood, using on Marsyas for feature extraction and libSVM library for classification and regression. Several experimental tests were run to study different approaches and the respective results.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Renato  Panda.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: January 14, 2021.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
